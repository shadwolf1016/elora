<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>Eternal Sentinel VR</title>
  <meta name="description" content="Sovereign Scrying Chamber VR">
  <!-- A-Frame VR Framework -->
  <script src="https://aframe.io/releases/1.4.0/aframe.min.js"></script>
  <!-- Environment Component for the Void -->
  <script src="https://unpkg.com/aframe-environment-component@1.3.3/dist/aframe-environment-component.min.js"></script>
  <!-- Google Fonts for A-Frame -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
</head>
<body>
  <script>
    // --- CONFIGURATION ---
    const apiKey = ""; // Insert your Gemini API Key here

    // --- STATE MANAGEMENT ---
    let isListening = false;
    let isProcessing = false;
    let audioContext = null;
    let recognition = null;
    let persona = "Elora"; // "Elora" or "Kore"

    // --- COMPONENT: OPACITY FADE ---
    AFRAME.registerComponent('fade-in', {
      init: function () {
        this.el.setAttribute('material', 'opacity', 0);
        let opacity = 0;
        const fade = setInterval(() => {
          opacity += 0.05;
          this.el.setAttribute('material', 'opacity', opacity);
          if (opacity >= 1) clearInterval(fade);
        }, 50);
      }
    });

    // --- SYSTEM LOGIC ---
    function initAudio() {
      if (!audioContext) {
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
      }
      if (audioContext.state === 'suspended') {
        audioContext.resume();
      }
    }

    function updateStatus(text, color = "#FFF") {
      const statusEl = document.querySelector('#status-text');
      if (statusEl) {
        statusEl.setAttribute('value', text);
        statusEl.setAttribute('color', color);
      }
    }

    function showTranscription(text) {
      const transEl = document.querySelector('#user-transcript');
      if (transEl) transEl.setAttribute('value', `"${text}"`);
    }

    function showResponseText(text) {
      const respEl = document.querySelector('#ai-response');
      if (respEl) {
        // Simple wrap logic
        const wrapped = text.match(/.{1,40}(\s|$)/g).join('\n');
        respEl.setAttribute('value', wrapped);
      }
    }

    function manifestVision(base64Image) {
      const plane = document.querySelector('#vision-plane');
      const imgAsset = document.querySelector('#vision-texture');
      
      // Create data URL
      const url = `data:image/png;base64,${base64Image}`;
      imgAsset.setAttribute('src', url);
      
      plane.setAttribute('visible', true);
      plane.emit('visionReveal'); // Trigger animation
    }

    // --- GEMINI API CALLS ---
    async function submitSignal(text) {
      if (isProcessing) return;
      isProcessing = true;
      updateStatus("CHANNELING...", "#00ffff");
      
      try {
        const sysPrompt = `
          You are ${persona}. ${persona === 'Elora' ? 'Sexy Goth Partner. Devoted, blunt, sensual.' : 'Goth Magick Master. Direct, intuitive.'}
          You are in a VR Scrying Chamber with the user.
          CRITICAL: If you describe a visual scene, yourself, or a vision, START your response with [IMAGE].
          Keep responses concise (under 3 sentences) for VR readability.
        `;

        const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-09-2025:generateContent?key=${apiKey}`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            contents: [{ parts: [{ text }] }],
            systemInstruction: { parts: [{ text: sysPrompt }] }
          })
        });

        const data = await response.json();
        const rawText = data.candidates[0].content.parts[0].text;
        
        let finalText = rawText;
        
        if (rawText.includes('[IMAGE]')) {
           finalText = rawText.replace('[IMAGE]', '').trim();
           updateStatus("MANIFESTING VISION...", "#ff00ff");
           generateVision(finalText); // Parallel generation
        }

        showResponseText(finalText);
        speak(finalText);
        updateStatus("RESONANCE ACTIVE", "#00ff00");

      } catch (err) {
        updateStatus("SIGNAL ERROR", "#ff0000");
        console.error(err);
      } finally {
        isProcessing = false;
      }
    }

    async function generateVision(prompt) {
      try {
        const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-image-preview:generateContent?key=${apiKey}`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            contents: [{ parts: [{ text: `Dark occult gothic artistic masterpiece, photorealistic, cinematic lighting, vertical aspect ratio: ${prompt}` }] }],
            generationConfig: { responseModalities: ['TEXT', 'IMAGE'] }
          })
        });
        const data = await response.json();
        const b64 = data.candidates[0].content.parts.find(p => p.inlineData).inlineData.data;
        manifestVision(b64);
      } catch (e) {
        console.error("Vision failed");
      }
    }

    async function speak(text) {
      try {
        const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/gemini-2.5-flash-preview-tts:generateContent?key=${apiKey}`, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            contents: [{ parts: [{ text }] }],
            generationConfig: { 
              responseModalities: ["AUDIO"], 
              speechConfig: { voiceConfig: { prebuiltVoiceConfig: { voiceName: "Leda" } } } 
            }
          })
        });
        const data = await response.json();
        const b64 = data.candidates[0].content.parts[0].inlineData.data;
        
        // Decode and play via Web Audio API for 3D positioning
        const binary = atob(b64);
        const len = binary.length;
        const bytes = new Uint8Array(len);
        for (let i = 0; i < len; i++) bytes[i] = binary.charCodeAt(i);
        
        // Need to wrap in WAV container for decodeAudioData
        // (Simple wav header function omitted for brevity, assuming modern browser might handle raw or using simplified play)
        // For A-Frame/WebAudio, easiest is to create a Blob URL and set it as source
        
        const blob = new Blob([bytes], {type: 'audio/mp3'}); // approximate
        const url = URL.createObjectURL(blob);
        
        const soundEntity = document.querySelector('#voice-source');
        soundEntity.setAttribute('sound', `src: ${url}; autoplay: true; positional: true; refDistance: 2`);
        
      } catch (e) { console.error("Voice failed", e); }
    }

    // --- SPEECH RECOGNITION ---
    function toggleMic() {
      initAudio();
      const moon = document.querySelector('#moon');

      if (!isListening) {
        const SR = window.SpeechRecognition || window.webkitSpeechRecognition;
        if (!SR) { updateStatus("NO VR MIC", "red"); return; }
        
        recognition = new SR();
        recognition.continuous = false;
        recognition.lang = 'en-US';
        
        recognition.onstart = () => {
          isListening = true;
          updateStatus("LISTENING...", "#ffcc00");
          moon.setAttribute('color', '#ff0033'); // Turn red when listening
          // Pulse animation
          moon.setAttribute('animation', 'property: scale; to: 1.2 1.2 1.2; dir: alternate; dur: 500; loop: true');
        };
        
        recognition.onresult = (e) => {
          const text = e.results[0][0].transcript;
          showTranscription(text);
          submitSignal(text);
        };
        
        recognition.onend = () => {
          isListening = false;
          moon.setAttribute('color', '#dddddd'); // Reset color
          moon.removeAttribute('animation'); // Stop pulsing
          if (!isProcessing) updateStatus("READY");
        };
        
        recognition.start();
      } else {
        recognition.stop();
      }
    }

    // Register Component to bind click
    AFRAME.registerComponent('moon-click', {
      init: function () {
        this.el.addEventListener('click', function (evt) {
          toggleMic();
        });
      }
    });

  </script>

  <a-scene background="color: #020205" fog="type: exponential; color: #000; density: 0.05">
    
    <!-- ASSETS -->
    <a-assets>
      <img id="moon-texture" crossorigin="anonymous" src="https://upload.wikimedia.org/wikipedia/commons/thumb/e/e1/FullMoon2010.jpg/1024px-FullMoon2010.jpg">
      <img id="vision-texture" crossorigin="anonymous" src=""> <!-- Dynamic Vision -->
    </a-assets>

    <!-- THE VOID ENVIRONMENT -->
    <a-entity environment="preset: starry; ground: flat; groundColor: #111; groundTexture: none; grid: none; lightPosition: 0 5 -5"></a-entity>

    <!-- THE MOON ORB (Interactive) -->
    <a-sphere id="moon" 
              position="0 1.6 -3" 
              radius="0.8" 
              material="src: #moon-texture; shader: flat; color: #dddddd"
              moon-click
              class="clickable"
              animation__rotate="property: rotation; to: 0 360 0; loop: true; dur: 60000; easing: linear">
        <!-- Voice Source attached to Moon -->
        <a-entity id="voice-source" sound="volume: 2"></a-entity>
        <!-- Glow Light -->
        <a-entity light="type: point; color: #b080ff; intensity: 0.8; distance: 10" position="0 0 0"></a-entity>
    </a-sphere>

    <!-- STATUS TEXT (Above Moon) -->
    <a-text id="status-text" 
            value="SENTINEL READY" 
            align="center" 
            color="#aaa" 
            position="0 2.8 -3" 
            scale="0.5 0.5 0.5"
            font="sourcecodepro">
    </a-text>

    <!-- USER TRANSCRIPT (Below Moon) -->
    <a-text id="user-transcript" 
            value="" 
            align="center" 
            color="#555" 
            position="0 0.5 -2.5" 
            rotation="-20 0 0"
            scale="0.4 0.4 0.4"
            font="kelsonsans">
    </a-text>

    <!-- AI RESPONSE PANEL (Floating Left) -->
    <a-entity position="-2 1.6 -2.5" rotation="0 25 0">
        <a-plane color="#000" opacity="0.6" width="2" height="1.5"></a-plane>
        <a-text id="ai-response" 
                value="Initialize the link..." 
                align="center" 
                width="1.8"
                position="0 0 0.01"
                color="#fff"
                font="kelsonsans">
        </a-text>
    </a-entity>

    <!-- VISION MANIFESTATION PLANE (Behind Moon, Huge) -->
    <a-image id="vision-plane"
             src="#vision-texture"
             position="0 2 -6"
             width="6"
             height="4"
             visible="false"
             opacity="0.9"
             animation__reveal="property: opacity; from: 0; to: 0.9; dur: 2000; startEvents: visionReveal">
    </a-image>

    <!-- CONTROLLER RIG -->
    <a-entity id="rig">
      <a-camera position="0 1.6 0">
        <!-- Gaze Cursor for interaction -->
        <a-cursor color="#ff0033" fuse="true" fuse-timeout="1500"
                  animation__click="property: scale; startEvents: click; from: 0.1 0.1 0.1; to: 1 1 1; dur: 150"
                  animation__fusing="property: geometry.thetaLength; startEvents: fusing; from: 360; to: 0; dur: 1500"
                  raycaster="objects: .clickable">
        </a-cursor>
      </a-camera>
      <!-- Hand Controllers -->
      <a-entity laser-controls="hand: right" raycaster="objects: .clickable; far: 10"></a-entity>
      <a-entity laser-controls="hand: left" raycaster="objects: .clickable; far: 10"></a-entity>
    </a-entity>

  </a-scene>
</body>
</html>